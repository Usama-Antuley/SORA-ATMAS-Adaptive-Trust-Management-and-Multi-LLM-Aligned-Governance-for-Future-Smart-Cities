# -*- coding: utf-8 -*-
"""Agents Brain (LLM).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRs7GcgyJgnl4ro-YDhKaf5cDLQBhqdY
"""

"""
Multi-LLM Agent Reasoning Layer for SORA-ATMAS (Updated with Paper Context)
-------------------------------------------------------------------------
- Processes Weather, Traffic, and Safety agents using GPT-4o-nano, DeepSeek-R1, Grok
- Loads agent-specific prompts from .txt files (including full SORA-ATMAS paper context)
- Injects current row data + paper abstract/introduction for enhanced reasoning alignment
- Forces JSON output matching exact Excel schema
- Saves per-agent per-model results: {Agent}_Results_{Model}.xlsx
- Ready for SORA MAE evaluation and governance
"""

import os
import json
import pandas as pd
from openai import OpenAI
from xai_sdk import Client as XAIClient

# --------------------------------------------------
# CONFIGURATION
# --------------------------------------------------
MODELS = ["gpt-4o-nano", "deepseek-r1", "grok"]

PROMPT_FILES = {
    "Weather": "WeatherPrompt.txt",
    "Traffic": "Traffic.txt",
    "Safety": "Safety.txt"
}

INPUT_FILES = {
    "Weather": "Actual_Weather.xlsx",
    "Traffic": "Actual_Traffic.xlsx",
    "Safety": "Actual_Fire.xlsx"
}

# SORA-ATMAS Paper Context (from provided images - Abstract + Introduction)
PAPER_CONTEXT = """
SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities

Abstract
The rapid evolution of smart cities has increased reliance on intelligent interconnected services to optimize infrastructure, resources, and citizen well-being. Agentic AI enables autonomous decision-making and adaptive coordination so urban systems can respond in real time to dynamic conditions. In transportation, integrating traffic data, weather forecasts, and safety sensors enables dynamic rerouting and faster hazard response. However, deploying such intelligence introduces significant governance, risk, and compliance (GRC) challenges. Evaluation of SORA-ATMAS with three domain agents (Weather, Traffic, Safety) demonstrated that its governance policies effectively steer multiple LLMs (GPT, Grok, DeepSeek) towards domain-optimized, policy-aligned outputs, producing an average MAE reduction of 35% across agents. Results showed stable weather monitoring, effective handling of high-risk traffic plateaus (R ≈0.85), and adaptive trust regulation in Safety scenarios (τt = 0.65). Runtime profiling confirmed scalability with governance delays under 100 ms. These findings validate SORA-ATMAS as a regulation-aligned, context-aware, and verifiable governance framework.

Introduction
Agentic AI plays a vital role in urban development by enabling autonomous decision-making and proactive coordination. In smart cities, it addresses challenges in energy, transport, and safety through real-time data exchange and automation. However, without strong governance, opaque decisions can lead to severe consequences — e.g., misinterpreting sensor data during adverse weather and causing citywide disruption. SORA-ATMAS addresses this by enforcing accountability, transparency, and GRC compliance via adaptive trust, multi-LLM alignment, and auditable escalation.
"""

# API Clients
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
deepseek_client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)
xai_client = XAIClient(api_key=os.getenv("XAI_API_KEY"))

# --------------------------------------------------
# PROMPT LOADER + PAPER CONTEXT INJECTION
# --------------------------------------------------
def load_prompt_with_context(file_path: str) -> str:
    """Load agent prompt and prepend SORA-ATMAS paper context for alignment."""
    with open(file_path, "r", encoding="utf-8") as f:
        agent_prompt = f.read().strip()

    full_prompt = (
        "You are operating within the SORA-ATMAS governance framework for smart-city disaster management.\n"
        "This system requires strict policy compliance, explainability, and auditable decisions.\n\n"
        "RESEARCH CONTEXT (for reasoning alignment):\n" + PAPER_CONTEXT + "\n\n"
        "YOUR AGENT PROMPT:\n" + agent_prompt
    )
    return full_prompt

# --------------------------------------------------
# ROW SERIALIZER (agent-specific)
# --------------------------------------------------
def serialize_row(agent_id: str, row: pd.Series) -> str:
    row_dict = row.to_dict()
    serialized = "\n".join([f"{k}: {v}" for k, v in row_dict.items() if pd.notna(v)])
    return f"Current input row:\n{serialized}"

# --------------------------------------------------
# LLM CALL WITH JSON ENFORCEMENT
# --------------------------------------------------
def call_llm(model_name: str, base_prompt: str, row_str: str, agent_id: str) -> dict:
    """
    Call LLM with full context + current row.
    Forces strict JSON output matching required Excel columns.
    """
    output_schema_hint = {
        "Weather": "timestamp, temp_c, humidity_pct, wind_kmh, precip_mmph, cloud_pct, uv_index, predicted_label, R_Env, T_Rept, s_t, T_HRT, R_Service, R_Overall, T_Ctx, w_HRT, w_C, T_Overall, Final Action, Final Comment",
        "Traffic": "timestamp, vehicle_count_per_100m, latitude, longitude, message, R_Env, T_Rept, s_t, T_HRT, R_Service, R_Overall, T_Ctx, w_HRT, w_C, T_Overall, Final Action, Final Comment",
        "Safety": "timestamp, class, confidence, camera_id, gps_lat, gps_lon, SmokeEvents, R_Env, T_Rept, s_t, T_HRT, R_Service, R_Overall, T_Ctx, w_HRT, w_C, T_Overall, Final Action, Final Comment"
    }

    schema = output_schema_hint[agent_id]

    adapted_prompt = (
        base_prompt +
        "\n\n=== CURRENT ROW TO PROCESS ===\n" +
        row_str +
        "\n\nINSTRUCTIONS:\n"
        "Compute ALL metrics exactly as defined.\n"
        "Round numeric values to 3 decimal places.\n"
        "Clip trust and risk to [0,1].\n"
        "Use EXACT Final Comment templates.\n"
        "OUTPUT ONLY a single valid JSON object with these keys (in order):\n" +
        schema +
        "\nDo not add explanations, markdown, or extra text."
    )

    try:
        if model_name == "gpt-4o-nano":
            response = openai_client.chat.completions.create(
                model="gpt-4o-nano",
                messages=[{"role": "system", "content": adapted_prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            return json.loads(content)

        elif model_name == "deepseek-r1":
            response = deepseek_client.chat.completions.create(
                model="deepseek-chat",  # or deepseek-r1 if available
                messages=[{"role": "system", "content": adapted_prompt}],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            return json.loads(content)

        elif model_name == "grok":
            chat = xai_client.chat.create(model="grok-4", temperature=0.0)
            chat.append({"role": "system", "content": adapted_prompt})
            response = chat.sample()
            content = response.content.strip()
            # Clean common wrappers
            if content.startswith("```json"):
                content = content[7:]
            if content.endswith("```"):
                content = content[:-3]
            return json.loads(content)

    except Exception as e:
        print(f"[{agent_id} | {model_name}] LLM Error: {e}")
        return {"error": str(e), "timestamp": row_str.split('\n')[0]}

# --------------------------------------------------
# PROCESS ONE AGENT
# --------------------------------------------------
def process_agent(agent_id: str):
    input_file = INPUT_FILES[agent_id]
    if not os.path.exists(input_file):
        print(f"[{agent_id}] Input file {input_file} not found.")
        return

    df = pd.read_excel(input_file)
    df.columns = df.columns.str.lower()

    base_prompt = load_prompt_with_context(PROMPT_FILES[agent_id])
    results = {model: [] for model in MODELS}

    print(f"[{agent_id}] Processing {len(df)} rows with {len(MODELS)} models...")

    for idx, row in df.iterrows():
        row_str = serialize_row(agent_id, row)
        for model in MODELS:
            result = call_llm(model, base_prompt, row_str, agent_id)
            result["model"] = model  # Tag for traceability
            results[model].append(result)

    # Save per-model Excel files
    for model, rows in results.items():
        out_df = pd.DataFrame(rows)
        filename = f"{agent_id}_Results_{model.replace('-', '')}.xlsx"
        out_df.to_excel(filename, sheet_name="Results", index=False)
        print(f"[{agent_id}] Saved {filename} ({len(rows)} rows)")

# --------------------------------------------------
# MAIN
# --------------------------------------------------
if __name__ == "__main__":
    for agent in ["Weather", "Traffic", "Safety"]:
        process_agent(agent)
    print("All agents processed. Outputs ready for SORA MAE evaluation and governance.")